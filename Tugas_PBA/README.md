# Hasil dan Analisi

Model BERT (Bidirectional Encoder Representations from Transformers) menunjukkan performa yang kuat dalam memahami konteks dua arah dalam teks. Berdasarkan hasil simulasi, training loss menurun dengan signifikan selama tiga epoch pertama, menandakan bahwa model mampu mempelajari pola data secara efektif. Validation loss mencapai nilai minimum pada epoch ke-4 (0.288), yang menunjukkan performa optimal, namun mulai meningkat pada epoch ke-5 (0.295), menandakan risiko overfitting jika pelatihan dilanjutkan. Validation accuracy meningkat pesat dari 88% pada epoch pertama hingga mencapai 92% pada epoch ketiga, dan stabil hingga epoch ke-4 sebelum sedikit menurun pada epoch ke-5. Dengan akurasi yang tinggi, BERT membuktikan kemampuannya dalam generalisasi, khususnya pada dataset seperti ags_news_csv, yang memiliki teks berita berstruktur baik.

Dalam konteks kebutuhan dataset, BERT memerlukan dataset yang lebih besar dibandingkan model lain seperti HAN untuk memaksimalkan performanya. Hal ini dikarenakan BERT dioptimalkan melalui pretraining pada data berskala besar seperti Wikipedia dan BookCorpus. Untuk tugas fine-tuning, dataset ags_news_csv cukup memadai, namun dataset yang lebih besar akan meningkatkan kemampuan generalisasi, terutama pada domain yang heterogen.

Dari segi waktu dan sumber daya komputasi, BERT membutuhkan GPU atau TPU untuk menjalankan proses pelatihan dengan efisien, terutama karena embedding kontekstual yang dinamis membutuhkan memori besar dan komputasi intensif. Waktu pelatihan lebih panjang dibandingkan model seperti HAN, terutama jika dataset besar atau ukuran batch tinggi. Namun, hasil pelatihan yang lebih cepat dapat dicapai dengan memanfaatkan varian yang lebih ringan seperti DistilBERT.

Kemampuan generalisasi BERT sangat unggul, berkat embedding kontekstualnya yang menangkap makna kata dalam konteks yang lebih luas. Generalisasi ini terlihat pada akurasi validasi yang tinggi (92%), meskipun dataset latih berasal dari domain tertentu seperti teks berita. Dibandingkan dengan model lain, BERT lebih mampu memahami teks dengan pola kompleks, menjadikannya pilihan ideal untuk tugas klasifikasi teks dengan akurasi tinggi dan generalisasi yang baik. Namun, keunggulan ini harus diimbangi dengan pertimbangan sumber daya komputasi yang tersedia.

