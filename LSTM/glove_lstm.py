# -*- coding: utf-8 -*-
"""GLOVE-LSTM

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_3S4Y0svEkFzo8OUrjnYQp1kC5GnqZSm
"""

import numpy as np
import pandas as pd
import os
import tarfile
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from tqdm import tqdm
import re
from collections import Counter

# Extract AG News dataset
tar_path = "/content/ag_news_csv.tar.gz"
extracted_folder = "./ag_news_csv"

if not os.path.exists(extracted_folder):
    with tarfile.open(tar_path, "r:gz") as tar:
        tar.extractall(path=extracted_folder)
    print("Dataset extracted!")

# Load train dan test data
train_data = pd.read_csv("/content/ag_news_csv/ag_news_csv/train.csv")
test_data = pd.read_csv("/content/ag_news_csv/ag_news_csv/test.csv")

# Tampilkan contoh data
print(train_data.head())
print(test_data.head())

# Rename columns for clarity
train_data.columns = ["label", "title", "description"]
test_data.columns = ["label", "title", "description"]

print("Train data sample:")
print(train_data.head())

# Combine title and description for text input
train_data["text"] = train_data["title"] + " " + train_data["description"]
test_data["text"] = test_data["title"] + " " + test_data["description"]

def clean_text(text):
    text = text.lower()  # Convert ke lowercase
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text)  # Hapus karakter spesial
    text = text.strip()
    return text

train_data['text'] = train_data['text'].apply(clean_text)
test_data['text'] = test_data['text'].apply(clean_text)

def load_glove_embeddings(glove_path, embedding_dim=300):
    embeddings_index = {}
    with open(glove_path, 'r', encoding='utf-8') as f:
        for line in f:
            values = line.split()
            word = values[0]
            vector = np.asarray(values[1:], dtype='float32')
            embeddings_index[word] = vector
    print(f"Loaded {len(embeddings_index)} word vectors from GloVe.")
    return embeddings_index

# Path ke file GloVe
glove_path = '/content/glove.6B.100d.txt'
glove_embeddings = load_glove_embeddings(glove_path, embedding_dim=300)

# Tokenize text dan buat vocabulary
def tokenize(texts):
    return [text.split() for text in texts]

train_tokens = tokenize(train_data['text'])
test_tokens = tokenize(test_data['text'])

# Buat word-to-index
vocab = Counter(word for tokens in train_tokens for word in tokens)
word2idx = {word: idx + 1 for idx, (word, _) in enumerate(vocab.items())}  # Mulai dari 1
word2idx['<PAD>'] = 0  # Padding index
idx2word = {idx: word for word, idx in word2idx.items()}

# Buat embedding matrix
embedding_dim = 100
vocab_size = len(word2idx)
embedding_matrix = np.zeros((vocab_size, embedding_dim))

for word, idx in word2idx.items():
    vector = glove_embeddings.get(word)
    if vector is not None:
        embedding_matrix[idx] = vector

embedding_matrix = torch.FloatTensor(embedding_matrix)
print("Embedding matrix shape:", embedding_matrix.shape)

class TextDataset(Dataset):
    def __init__(self, texts, labels, word2idx, max_len=50):
        self.texts = [[word2idx.get(word, 0) for word in text[:max_len]] for text in texts]
        self.labels = labels
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        x = self.texts[idx]
        x = x + [0] * (self.max_len - len(x))  # Padding
        y = self.labels[idx] - 1  # Ubah label agar dimulai dari 0
        return torch.tensor(x), torch.tensor(y)

# Split train dan validation data
train_texts, val_texts, train_labels, val_labels = train_test_split(
    train_tokens, train_data['label'].values, test_size=0.2, random_state=42
)

train_dataset = TextDataset(train_texts, train_labels, word2idx)
val_dataset = TextDataset(val_texts, val_labels, word2idx)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=64)

class LSTMClassifier(nn.Module):
    def __init__(self, embedding_matrix, hidden_dim, output_dim):
        super(LSTMClassifier, self).__init__()
        num_embeddings, embedding_dim = embedding_matrix.shape
        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=True)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)
        self.dropout = nn.Dropout(0.5)

    def forward(self, x):
        x = self.embedding(x)
        _, (hidden, _) = self.lstm(x)
        return self.fc(self.dropout(hidden[-1]))

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = LSTMClassifier(embedding_matrix, hidden_dim=128, output_dim=4).to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Evaluate Function
def evaluate(model, loader, criterion):
    model.eval()
    loss = 0
    with torch.no_grad():
        for x, y in loader:
            x, y = x.to(device), y.to(device)
            outputs = model(x)
            loss += criterion(outputs, y).item()
    return loss / len(loader)

# Training Loop
for epoch in range(5):
    model.train()
    total_loss = 0
    for x, y in tqdm(train_loader):
        x, y = x.to(device), y.to(device)
        optimizer.zero_grad()
        outputs = model(x)
        loss = criterion(outputs, y)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    val_loss = evaluate(model, val_loader, criterion)
    print(f"Epoch {epoch+1}, Train Loss: {total_loss/len(train_loader):.4f}, Val Loss: {val_loss:.4f}")

test_dataset = TextDataset(test_tokens, test_data['label'].values, word2idx)
test_loader = DataLoader(test_dataset, batch_size=64)

correct, total = 0, 0
model.eval()
with torch.no_grad():
    for x, y in test_loader:
        x, y = x.to(device), y.to(device)
        outputs = model(x)
        preds = torch.argmax(outputs, dim=1)
        correct += (preds == y).sum().item()
        total += y.size(0)

print(f"Test Accuracy: {correct / total:.4f}")